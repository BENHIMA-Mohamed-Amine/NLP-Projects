{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QFkyeS7X6tug"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "doU_4U7u_qdK"
      },
      "outputs": [],
      "source": [
        "for gpu in tf.config.experimental.list_physical_devices('GPU'):\n",
        "    tf.config.experimental.set_memory_growth(gpu, True)\n",
        "\n",
        "def positional_encoding(length: int, depth: int):\n",
        "    \"\"\"\n",
        "    Generates a positional encoding for a given length and depth.\n",
        "\n",
        "    Args:\n",
        "        length (int): The length of the input sequence.\n",
        "        depth (int): The depth that represents the dimensionality of the encoding.\n",
        "\n",
        "    Returns:\n",
        "        tf.Tensor: The positional encoding of shape (length, depth).\n",
        "    \"\"\"\n",
        "    depth = depth / 2\n",
        "\n",
        "    positions = np.arange(length)[:, np.newaxis]     # (seq, 1)m\n",
        "    depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n",
        "\n",
        "    angle_rates = 1 / (10000**depths)         # (1, depth)\n",
        "    angle_rads = positions * angle_rates      # (pos, depth)\n",
        "\n",
        "    pos_encoding = np.concatenate([np.sin(angle_rads), np.cos(angle_rads)], axis=-1)\n",
        "\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "EAQP8Na1AwIe"
      },
      "outputs": [],
      "source": [
        "class PositionalEmbedding(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    A positional embedding layer combines the input embedding with a positional encoding that helps the Transformer\n",
        "    to understand the relative position of the input tokens. This layer takes the input of tokens and converts them\n",
        "    into sequence of embeddings vector. Then, it adds the positional encoding to the embeddings.\n",
        "\n",
        "    Methods:\n",
        "        compute_mask: Computes the mask to be applied to the embeddings.\n",
        "        call: Performs the forward pass of the layer.\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size: int, d_model: int, embedding: tf.keras.layers.Embedding=None):\n",
        "        \"\"\" Constructor of the PositionalEmbedding layer.\n",
        "\n",
        "        Args:\n",
        "            vocab_size (int): The size of the vocabulary. I. e. the number of unique tokens in the input sequence.\n",
        "            d_model (int): The dimensionality of the embedding vector.\n",
        "            embedding (tf.keras.layers.Embedding): The custom embedding layer. If None, a default embedding layer will be created.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True) if embedding is None else embedding\n",
        "        self.pos_encoding = positional_encoding(length=2048, depth=d_model)\n",
        "\n",
        "    def compute_mask(self, *args, **kwargs):\n",
        "        \"\"\" Computes the mask to be applied to the embeddings.\n",
        "        \"\"\"\n",
        "        return self.embedding.compute_mask(*args, **kwargs)\n",
        "\n",
        "    def call(self, x: tf.Tensor) -> tf.Tensor:\n",
        "        \"\"\" Performs the forward pass of the layer.\n",
        "\n",
        "        Args:\n",
        "            x (tf.Tensor): The input tensor of shape (batch_size, seq_length).\n",
        "\n",
        "        Returns:\n",
        "            tf.Tensor: The output sequence of embedding vectors with added positional information. The shape is\n",
        "                (batch_size, seq_length, d_model).\n",
        "        \"\"\"\n",
        "        x = self.embedding(x)\n",
        "        length = tf.shape(x)[1]\n",
        "        # This factor sets the relative scale of the embedding and positonal_encoding.\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x = x + self.pos_encoding[tf.newaxis, :length, :]\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGmxk3BADKzd",
        "outputId": "bfdf23d0-bc95-46db-d65a-ece1c0e06a46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "random_input shape (1, 100)\n",
            "PositionalEmbedding output (1, 100, 512)\n"
          ]
        }
      ],
      "source": [
        "vocab_size = 1000\n",
        "d_model = 512\n",
        "\n",
        "embedding_layer = PositionalEmbedding(vocab_size, d_model)\n",
        "\n",
        "random_input = np.random.randint(0, vocab_size, size=(1, 100))\n",
        "\n",
        "output = embedding_layer(random_input)\n",
        "print(\"random_input shape\", random_input.shape)\n",
        "print(\"PositionalEmbedding output\", output.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "TcJi7-TeET8t"
      },
      "outputs": [],
      "source": [
        "class BaseAttention(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Base class for all attention layers. It contains the common functionality of all attention layers.\n",
        "    This layer contains a MultiHeadAttention layer, a LayerNormalization layer and an Add layer.\n",
        "    It is used as a base class for the GlobalSelfAttention, CausalSelfAttention and CrossAttention layers.\n",
        "    And it is not intended to be used directly.\n",
        "\n",
        "    Methods:\n",
        "        call: Performs the forward pass of the layer.\n",
        "\n",
        "    Attributes:\n",
        "        mha (tf.keras.layers.MultiHeadAttention): The MultiHeadAttention layer.\n",
        "        layernorm (tf.keras.layers.LayerNormalization): The LayerNormalization layer.\n",
        "        add (tf.keras.layers.Add): The Add layer.\n",
        "    \"\"\"\n",
        "    def __init__(self, **kwargs: dict):\n",
        "        \"\"\" Constructor of the BaseAttention layer.\n",
        "\n",
        "        Args:\n",
        "            **kwargs: Additional keyword arguments that are passed to the MultiHeadAttention layer, e. g.\n",
        "                        num_heads (number of heads), key_dim (dimensionality of the key space), etc.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
        "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "        self.add = tf.keras.layers.Add()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xxRO2pjKLbjf"
      },
      "outputs": [],
      "source": [
        "class CrossAttention(BaseAttention):\n",
        "    \"\"\"\n",
        "    A class that implements the cross-attention layer by inheriting from the BaseAttention class.\n",
        "    This layer is used to process two different sequences and attends to the context sequence while processing the query sequence.\n",
        "\n",
        "    Methods:\n",
        "        call: Performs the forward pass of the layer.\n",
        "\n",
        "    Attributes:\n",
        "        mha (tf.keras.layers.MultiHeadAttention): The MultiHeadAttention layer.\n",
        "        layernorm (tf.keras.layers.LayerNormalization): The LayerNormalization layer.\n",
        "        add (tf.keras.layers.Add): The Add layer.\n",
        "    \"\"\"\n",
        "    def call(self, x: tf.Tensor, context: tf.Tensor) -> tf.Tensor:\n",
        "        \"\"\"\n",
        "        The call function that performs the cross-attention operation.\n",
        "\n",
        "        Args:\n",
        "            x (tf.Tensor): The query (expected Transformer results) sequence of shape (batch_size, seq_length, d_model).\n",
        "            context (tf.Tensor): The context (inputs to the Encoder layer) sequence of shape (batch_size, seq_length, d_model).\n",
        "\n",
        "        Returns:\n",
        "            tf.Tensor: The output sequence of shape (batch_size, seq_length, d_model).\n",
        "        \"\"\"\n",
        "        attn_output, attn_scores = self.mha(query=x, key=context, value=context, return_attention_scores=True)\n",
        "\n",
        "        # Cache the attention scores for plotting later.\n",
        "        self.last_attn_scores = attn_scores\n",
        "\n",
        "        x = self.add([x, attn_output])\n",
        "        x = self.layernorm(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "haO0NMfhLnYE",
        "outputId": "473759f5-5f03-4b70-83e4-4c4ac95694c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "encoder_embeddings shape (1, 100, 512)\n",
            "decoder_embeddings shape (1, 110, 512)\n",
            "cross_attention_output shape (1, 110, 512)\n"
          ]
        }
      ],
      "source": [
        "encoder_vocab_size = 1000\n",
        "decoder_vocab_size = 1100\n",
        "d_model = 512\n",
        "\n",
        "encoder_embedding_layer = PositionalEmbedding(vocab_size, d_model)\n",
        "decoder_embedding_layer = PositionalEmbedding(vocab_size, d_model)\n",
        "\n",
        "random_encoder_input = np.random.randint(0, encoder_vocab_size, size=(1, 100))\n",
        "random_decoder_input = np.random.randint(0, decoder_vocab_size, size=(1, 110))\n",
        "\n",
        "encoder_embeddings = encoder_embedding_layer(random_encoder_input)\n",
        "decoder_embeddings = decoder_embedding_layer(random_decoder_input)\n",
        "\n",
        "print(\"encoder_embeddings shape\", encoder_embeddings.shape)\n",
        "print(\"decoder_embeddings shape\", decoder_embeddings.shape)\n",
        "\n",
        "cross_attention_layer = CrossAttention(num_heads=2, key_dim=512)\n",
        "cross_attention_output = cross_attention_layer(decoder_embeddings, encoder_embeddings)\n",
        "\n",
        "print(\"cross_attention_output shape\", cross_attention_output.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "zqVZnllcL9Z0"
      },
      "outputs": [],
      "source": [
        "class GlobalSelfAttention(BaseAttention):\n",
        "    \"\"\"\n",
        "    A class that implements the global self-attention layer by inheriting from the BaseAttention class.\n",
        "    This layer is used to process a single sequence and attends to all the tokens in the sequence.\n",
        "\n",
        "    Methods:\n",
        "        call: Performs the forward pass of the layer.\n",
        "\n",
        "    Attributes:\n",
        "        mha (tf.keras.layers.MultiHeadAttention): The MultiHeadAttention layer.\n",
        "        layernorm (tf.keras.layers.LayerNormalization): The LayerNormalization layer.\n",
        "        add (tf.keras.layers.Add): The Add layer.\n",
        "    \"\"\"\n",
        "    def call(self, x: tf.Tensor) -> tf.Tensor:\n",
        "        \"\"\"\n",
        "        The call function that performs the global self-attention operation.\n",
        "\n",
        "        Args:\n",
        "            x (tf.Tensor): The input sequence of shape (batch_size, seq_length, d_model).\n",
        "\n",
        "        Returns:\n",
        "            tf.Tensor: The output sequence of shape (batch_size, seq_length, d_model).\n",
        "        \"\"\"\n",
        "        attn_output = self.mha(query=x, value=x, key=x)\n",
        "        x = self.add([x, attn_output])\n",
        "        x = self.layernorm(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGKiLBY2MRr2",
        "outputId": "a0b0e308-cb38-435d-b7a6-980e93497481"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "encoder_embeddings shape (1, 100, 512)\n",
            "global_self_attention_output shape (1, 100, 512)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "encoder_vocab_size = 1000\n",
        "d_model = 512\n",
        "\n",
        "encoder_embedding_layer = PositionalEmbedding(vocab_size, d_model)\n",
        "\n",
        "random_encoder_input = np.random.randint(0, encoder_vocab_size, size=(1, 100))\n",
        "\n",
        "encoder_embeddings = encoder_embedding_layer(random_encoder_input)\n",
        "\n",
        "print(\"encoder_embeddings shape\", encoder_embeddings.shape)\n",
        "\n",
        "cross_attention_layer = GlobalSelfAttention(num_heads=2, key_dim=512)\n",
        "cross_attention_output = cross_attention_layer(encoder_embeddings)\n",
        "\n",
        "print(\"global_self_attention_output shape\", cross_attention_output.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "pl8QeXY7MWR0"
      },
      "outputs": [],
      "source": [
        "class CausalSelfAttention(BaseAttention):\n",
        "    \"\"\"\n",
        "    Call self attention on the input sequence, ensuring that each position in the\n",
        "    output depends only on previous positions (i.e. a causal model).\n",
        "\n",
        "    Methods:\n",
        "        call: Performs the forward pass of the layer.\n",
        "\n",
        "    Attributes:\n",
        "        mha (tf.keras.layers.MultiHeadAttention): The MultiHeadAttention layer.\n",
        "        layernorm (tf.keras.layers.LayerNormalization): The LayerNormalization layer.\n",
        "        add (tf.keras.layers.Add): The Add layer.\n",
        "    \"\"\"\n",
        "    def call(self, x: tf.Tensor) -> tf.Tensor:\n",
        "        \"\"\"\n",
        "        The call function that performs the causal self-attention operation.\n",
        "\n",
        "        Args:\n",
        "            x (tf.Tensor): The input sequence of shape (batch_size, seq_length, d_model).\n",
        "\n",
        "        Returns:\n",
        "            tf.Tensor: The output sequence of shape (batch_size, seq_length, d_model).\n",
        "        \"\"\"\n",
        "        attn_output = self.mha(query=x, value=x, key=x, use_causal_mask = True)\n",
        "        x = self.add([x, attn_output])\n",
        "        x = self.layernorm(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HooO0TCdMddl",
        "outputId": "543b8b94-541e-4b1b-a202-d3a1df317244"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "decoder_embeddings shape (1, 110, 512)\n",
            "causal_self_attention_output shape (1, 110, 512)\n",
            "Difference between the two outputs: 0.0\n"
          ]
        }
      ],
      "source": [
        "decoder_vocab_size = 1100\n",
        "d_model = 512\n",
        "\n",
        "decoder_embedding_layer = PositionalEmbedding(vocab_size, d_model)\n",
        "\n",
        "random_decoder_input = np.random.randint(0, decoder_vocab_size, size=(1, 110))\n",
        "\n",
        "decoder_embeddings = decoder_embedding_layer(random_decoder_input)\n",
        "\n",
        "print(\"decoder_embeddings shape\", decoder_embeddings.shape)\n",
        "\n",
        "causal_self_attention_layer = CausalSelfAttention(num_heads=2, key_dim=512)\n",
        "causal_self_attention_output = causal_self_attention_layer(decoder_embeddings)\n",
        "\n",
        "print(\"causal_self_attention_output shape\", causal_self_attention_output.shape)\n",
        "\n",
        "out1 = causal_self_attention_layer(decoder_embedding_layer(random_decoder_input[:, :50])) # Only the first 50 tokens beffore applying the embedding layer\n",
        "out2 = causal_self_attention_layer(decoder_embedding_layer(random_decoder_input)[:, :50]) # Only the first 50 tokens after applying the embedding layer\n",
        "\n",
        "diff = tf.reduce_max(tf.abs(out1 - out2)).numpy()\n",
        "\n",
        "print(\"Difference between the two outputs:\", diff)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "LEBXUuCqMgh3"
      },
      "outputs": [],
      "source": [
        "class FeedForward(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    A class that implements the feed-forward layer.\n",
        "\n",
        "    Methods:\n",
        "        call: Performs the forward pass of the layer.\n",
        "\n",
        "    Attributes:\n",
        "        seq (tf.keras.Sequential): The sequential layer that contains the feed-forward layers. It applies the two feed-forward layers and the dropout layer.\n",
        "        add (tf.keras.layers.Add): The Add layer.\n",
        "        layer_norm (tf.keras.layers.LayerNormalization): The LayerNormalization layer.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model: int, dff: int, dropout_rate: float=0.1):\n",
        "        \"\"\"\n",
        "        Constructor of the FeedForward layer.\n",
        "\n",
        "        Args:\n",
        "            d_model (int): The dimensionality of the model.\n",
        "            dff (int): The dimensionality of the feed-forward layer.\n",
        "            dropout_rate (float): The dropout rate.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.seq = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(dff, activation='relu'),\n",
        "            tf.keras.layers.Dense(d_model),\n",
        "            tf.keras.layers.Dropout(dropout_rate)\n",
        "        ])\n",
        "        self.add = tf.keras.layers.Add()\n",
        "        self.layer_norm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "    def call(self, x: tf.Tensor) -> tf.Tensor:\n",
        "        \"\"\"\n",
        "        The call function that performs the feed-forward operation.\n",
        "\n",
        "        Args:\n",
        "            x (tf.Tensor): The input sequence of shape (batch_size, seq_length, d_model).\n",
        "\n",
        "        Returns:\n",
        "            tf.Tensor: The output sequence of shape (batch_size, seq_length, d_model).\n",
        "        \"\"\"\n",
        "        x = self.add([x, self.seq(x)])\n",
        "        x = self.layer_norm(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R0HAI_spMpH9",
        "outputId": "87381f9e-e292-44a5-ba5f-18c74ee4b9fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "encoder_embeddings shape (1, 100, 512)\n",
            "feed_forward_output shape (1, 100, 512)\n"
          ]
        }
      ],
      "source": [
        "encoder_vocab_size = 1000\n",
        "d_model = 512\n",
        "\n",
        "encoder_embedding_layer = PositionalEmbedding(vocab_size, d_model)\n",
        "\n",
        "random_encoder_input = np.random.randint(0, encoder_vocab_size, size=(1, 100))\n",
        "\n",
        "encoder_embeddings = encoder_embedding_layer(random_encoder_input)\n",
        "\n",
        "print(\"encoder_embeddings shape\", encoder_embeddings.shape)\n",
        "\n",
        "feed_forward_layer = FeedForward(d_model, dff=2048)\n",
        "feed_forward_output = feed_forward_layer(encoder_embeddings)\n",
        "\n",
        "print(\"feed_forward_output shape\", feed_forward_output.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "29Ohv8eAM5p9"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    A single layer of the Encoder. Usually there are multiple layers stacked on top of each other.\n",
        "\n",
        "    Methods:\n",
        "        call: Performs the forward pass of the layer.\n",
        "\n",
        "    Attributes:\n",
        "        self_attention (GlobalSelfAttention): The global self-attention layer.\n",
        "        ffn (FeedForward): The feed-forward layer.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model: int, num_heads: int, dff: int, dropout_rate: float=0.1):\n",
        "        \"\"\"\n",
        "        Constructor of the EncoderLayer.\n",
        "\n",
        "        Args:\n",
        "            d_model (int): The dimensionality of the model.\n",
        "            num_heads (int): The number of heads in the multi-head attention layer.\n",
        "            dff (int): The dimensionality of the feed-forward layer.\n",
        "            dropout_rate (float): The dropout rate.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.self_attention = GlobalSelfAttention(\n",
        "            num_heads=num_heads,\n",
        "            key_dim=d_model,\n",
        "            dropout=dropout_rate\n",
        "            )\n",
        "\n",
        "        self.ffn = FeedForward(d_model, dff)\n",
        "\n",
        "    def call(self, x: tf.Tensor) -> tf.Tensor:\n",
        "        \"\"\"\n",
        "        The call function that performs the forward pass of the layer.\n",
        "\n",
        "        Args:\n",
        "            x (tf.Tensor): The input sequence of shape (batch_size, seq_length, d_model).\n",
        "\n",
        "        Returns:\n",
        "            tf.Tensor: The output sequence of shape (batch_size, seq_length, d_model).\n",
        "        \"\"\"\n",
        "        x = self.self_attention(x)\n",
        "        x = self.ffn(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ttf3cInxSOTf",
        "outputId": "1e6d7f21-8bf9-4836-95a8-fb0ecdbdbdad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "encoder_embeddings shape (1, 100, 512)\n",
            "encoder_layer_output shape (1, 100, 512)\n"
          ]
        }
      ],
      "source": [
        "encoder_vocab_size = 1000\n",
        "d_model = 512\n",
        "\n",
        "encoder_embedding_layer = PositionalEmbedding(vocab_size, d_model)\n",
        "\n",
        "random_encoder_input = np.random.randint(0, encoder_vocab_size, size=(1, 100))\n",
        "\n",
        "encoder_embeddings = encoder_embedding_layer(random_encoder_input)\n",
        "\n",
        "print(\"encoder_embeddings shape\", encoder_embeddings.shape)\n",
        "\n",
        "encoder_layer = EncoderLayer(d_model, num_heads=2, dff=2048)\n",
        "\n",
        "encoder_layer_output = encoder_layer(encoder_embeddings)\n",
        "\n",
        "print(\"encoder_layer_output shape\", encoder_layer_output.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "7ODCOgM_SRCL"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    A custom TensorFlow layer that implements the Encoder. This layer is mostly used in the Transformer models\n",
        "    for natural language processing tasks, such as machine translation, text summarization or text classification.\n",
        "\n",
        "    Methods:\n",
        "        call: Performs the forward pass of the layer.\n",
        "\n",
        "    Attributes:\n",
        "        d_model (int): The dimensionality of the model.\n",
        "        num_layers (int): The number of layers in the encoder.\n",
        "        pos_embedding (PositionalEmbedding): The positional embedding layer.\n",
        "        enc_layers (list): The list of encoder layers.\n",
        "        dropout (tf.keras.layers.Dropout): The dropout layer.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_layers: int, d_model: int, num_heads: int, dff: int, vocab_size: int, dropout_rate: float=0.1):\n",
        "        \"\"\"\n",
        "        Constructor of the Encoder.\n",
        "\n",
        "        Args:\n",
        "            num_layers (int): The number of layers in the encoder.\n",
        "            d_model (int): The dimensionality of the model.\n",
        "            num_heads (int): The number of heads in the multi-head attention layer.\n",
        "            dff (int): The dimensionality of the feed-forward layer.\n",
        "            vocab_size (int): The size of the vocabulary.\n",
        "            dropout_rate (float): The dropout rate.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size, d_model=d_model)\n",
        "\n",
        "        self.enc_layers = [\n",
        "            EncoderLayer(d_model=d_model,\n",
        "                        num_heads=num_heads,\n",
        "                        dff=dff,\n",
        "                        dropout_rate=dropout_rate)\n",
        "            for _ in range(num_layers)]\n",
        "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, x: tf.Tensor) -> tf.Tensor:\n",
        "        \"\"\"\n",
        "        The call function that performs the forward pass of the layer.\n",
        "\n",
        "        Args:\n",
        "            x (tf.Tensor): The input sequence of shape (batch_size, seq_length).\n",
        "\n",
        "        Returns:\n",
        "            tf.Tensor: The output sequence of shape (batch_size, seq_length, d_model).\n",
        "        \"\"\"\n",
        "        x = self.pos_embedding(x)\n",
        "        # here x has shape `(batch_size, seq_len, d_model)`\n",
        "\n",
        "        # Add dropout.\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x)\n",
        "\n",
        "        return x  # Shape `(batch_size, seq_len, d_model)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tp1lT15WSUzb",
        "outputId": "598af620-cbee-4ca5-f838-cb43070d3952"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "random_encoder_input shape (1, 100)\n",
            "encoder_output shape (1, 100, 512)\n"
          ]
        }
      ],
      "source": [
        "encoder_vocab_size = 1000\n",
        "d_model = 512\n",
        "\n",
        "encoder = Encoder(num_layers=2, d_model=d_model, num_heads=2, dff=2048, vocab_size=encoder_vocab_size)\n",
        "\n",
        "random_encoder_input = np.random.randint(0, encoder_vocab_size, size=(1, 100))\n",
        "\n",
        "encoder_output = encoder(random_encoder_input)\n",
        "\n",
        "print(\"random_encoder_input shape\", random_encoder_input.shape)\n",
        "print(\"encoder_output shape\", encoder_output.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "VioJ6bQrSZMI"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    A single layer of the Decoder. Usually there are multiple layers stacked on top of each other.\n",
        "\n",
        "    Methods:\n",
        "        call: Performs the forward pass of the layer.\n",
        "\n",
        "    Attributes:\n",
        "        causal_self_attention (CausalSelfAttention): The causal self-attention layer.\n",
        "        cross_attention (CrossAttention): The cross-attention layer.\n",
        "        ffn (FeedForward): The feed-forward layer.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model: int, num_heads: int, dff: int, dropout_rate: float=0.1):\n",
        "        \"\"\"\n",
        "        Constructor of the DecoderLayer.\n",
        "\n",
        "        Args:\n",
        "            d_model (int): The dimensionality of the model.\n",
        "            num_heads (int): The number of heads in the multi-head attention layer.\n",
        "            dff (int): The dimensionality of the feed-forward layer.\n",
        "            dropout_rate (float): The dropout rate.\n",
        "        \"\"\"\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.causal_self_attention = CausalSelfAttention(\n",
        "            num_heads=num_heads,\n",
        "            key_dim=d_model,\n",
        "            dropout=dropout_rate)\n",
        "\n",
        "        self.cross_attention = CrossAttention(\n",
        "            num_heads=num_heads,\n",
        "            key_dim=d_model,\n",
        "            dropout=dropout_rate)\n",
        "\n",
        "        self.ffn = FeedForward(d_model, dff)\n",
        "\n",
        "    def call(self, x: tf.Tensor, context: tf.Tensor) -> tf.Tensor:\n",
        "        \"\"\"\n",
        "        The call function that performs the forward pass of the layer.\n",
        "\n",
        "        Args:\n",
        "            x (tf.Tensor): The input sequence of shape (batch_size, seq_length, d_model). x is usually the output of the previous decoder layer.\n",
        "            context (tf.Tensor): The context sequence of shape (batch_size, seq_length, d_model). Context is usually the output of the encoder.\n",
        "        \"\"\"\n",
        "        x = self.causal_self_attention(x=x)\n",
        "        x = self.cross_attention(x=x, context=context)\n",
        "\n",
        "        # Cache the last attention scores for plotting later\n",
        "        self.last_attn_scores = self.cross_attention.last_attn_scores\n",
        "\n",
        "        x = self.ffn(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vuJ8p58UScoT",
        "outputId": "a1b880b8-db69-4434-d753-1539b3946782"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "random_decoder_input shape (1, 110)\n",
            "decoder_embeddings shape (1, 110, 512)\n",
            "decoder_output shape (1, 110, 512)\n"
          ]
        }
      ],
      "source": [
        "# Test DecoderLayer layer\n",
        "decoder_vocab_size = 1000\n",
        "d_model = 512\n",
        "dff = 2048\n",
        "num_heads = 8\n",
        "\n",
        "decoder_layer = DecoderLayer(d_model, num_heads, dff)\n",
        "\n",
        "random_decoderLayer_input = np.random.randint(0, decoder_vocab_size, size=(1, 110))\n",
        "\n",
        "decoder_embeddings = encoder_embedding_layer(random_decoderLayer_input)\n",
        "\n",
        "decoderLayer_output = decoder_layer(decoder_embeddings, encoder_output)\n",
        "\n",
        "print(\"random_decoder_input shape\", random_decoderLayer_input.shape)\n",
        "print(\"decoder_embeddings shape\", decoder_embeddings.shape)\n",
        "print(\"decoder_output shape\", decoderLayer_output.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "11p_KFArSe8p"
      },
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    A custom TensorFlow layer that implements the Decoder. This layer is mostly used in the Transformer models\n",
        "    for natural language processing tasks, such as machine translation, text summarization or text classification.\n",
        "\n",
        "    Methods:\n",
        "        call: Performs the forward pass of the layer.\n",
        "\n",
        "    Attributes:\n",
        "        d_model (int): The dimensionality of the model.\n",
        "        num_layers (int): The number of layers in the decoder.\n",
        "        pos_embedding (PositionalEmbedding): The positional embedding layer.\n",
        "        dec_layers (list): The list of decoder layers.\n",
        "        dropout (tf.keras.layers.Dropout): The dropout layer.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_layers: int, d_model: int, num_heads: int, dff: int, vocab_size: int, dropout_rate: float=0.1):\n",
        "        \"\"\"\n",
        "        Constructor of the Decoder.\n",
        "\n",
        "        Args:\n",
        "            num_layers (int): The number of layers in the decoder.\n",
        "            d_model (int): The dimensionality of the model.\n",
        "            num_heads (int): The number of heads in the multi-head attention layer.\n",
        "            dff (int): The dimensionality of the feed-forward layer.\n",
        "            vocab_size (int): The size of the vocabulary.\n",
        "            dropout_rate (float): The dropout rate.\n",
        "        \"\"\"\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size, d_model=d_model)\n",
        "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "        self.dec_layers = [\n",
        "            DecoderLayer(\n",
        "                d_model=d_model,\n",
        "                num_heads=num_heads,\n",
        "                dff=dff,\n",
        "                dropout_rate=dropout_rate) for _ in range(num_layers)]\n",
        "\n",
        "        self.last_attn_scores = None\n",
        "\n",
        "    def call(self, x: tf.Tensor, context: tf.Tensor) -> tf.Tensor:\n",
        "        \"\"\"\n",
        "        The call function that performs the forward pass of the layer.\n",
        "\n",
        "        Args:\n",
        "            x (tf.Tensor): The input sequence of shape (batch_size, target_seq_len).\n",
        "            context (tf.Tensor): The context sequence of shape (batch_size, input_seq_len, d_model).\n",
        "        \"\"\"\n",
        "        # `x` is token-IDs shape (batch, target_seq_len)\n",
        "        x = self.pos_embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x  = self.dec_layers[i](x, context)\n",
        "\n",
        "        self.last_attn_scores = self.dec_layers[-1].last_attn_scores\n",
        "\n",
        "        # The shape of x is (batch_size, target_seq_len, d_model).\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dc1foNiFSkMz",
        "outputId": "bd64c968-5bf2-4597-e060-6d0af8509cbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "random_decoder_input shape (1, 100)\n",
            "decoder_output shape (1, 100, 512)\n"
          ]
        }
      ],
      "source": [
        "# Test decoder layer\n",
        "decoder_vocab_size = 1000\n",
        "d_model = 512\n",
        "\n",
        "decoder_layer = Decoder(num_layers=2, d_model=d_model, num_heads=2, dff=2048, vocab_size=decoder_vocab_size)\n",
        "\n",
        "random_decoder_input = np.random.randint(0, decoder_vocab_size, size=(1, 100))\n",
        "\n",
        "decoder_output = decoder_layer(random_decoder_input, encoder_output)\n",
        "\n",
        "print(\"random_decoder_input shape\", random_decoder_input.shape)\n",
        "print(\"decoder_output shape\", decoder_output.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "EJUmZQUqSmJ1"
      },
      "outputs": [],
      "source": [
        "def Transformer(\n",
        "    input_vocab_size: int,\n",
        "    target_vocab_size: int,\n",
        "    encoder_input_size: int = None,\n",
        "    decoder_input_size: int = None,\n",
        "    num_layers: int=6,\n",
        "    d_model: int=512,\n",
        "    num_heads: int=8,\n",
        "    dff: int=2048,\n",
        "    dropout_rate: float=0.1,\n",
        "    ) -> tf.keras.Model:\n",
        "    \"\"\"\n",
        "    A custom TensorFlow model that implements the Transformer architecture.\n",
        "\n",
        "    Args:\n",
        "        input_vocab_size (int): The size of the input vocabulary.\n",
        "        target_vocab_size (int): The size of the target vocabulary.\n",
        "        encoder_input_size (int): The size of the encoder input sequence.\n",
        "        decoder_input_size (int): The size of the decoder input sequence.\n",
        "        num_layers (int): The number of layers in the encoder and decoder.\n",
        "        d_model (int): The dimensionality of the model.\n",
        "        num_heads (int): The number of heads in the multi-head attention layer.\n",
        "        dff (int): The dimensionality of the feed-forward layer.\n",
        "        dropout_rate (float): The dropout rate.\n",
        "\n",
        "    Returns:\n",
        "        A TensorFlow Keras model.\n",
        "    \"\"\"\n",
        "    inputs = [\n",
        "        tf.keras.layers.Input(shape=(encoder_input_size,), dtype=tf.int64),\n",
        "        tf.keras.layers.Input(shape=(decoder_input_size,), dtype=tf.int64)\n",
        "        ]\n",
        "\n",
        "    encoder_input, decoder_input = inputs\n",
        "\n",
        "    encoder = Encoder(num_layers=num_layers, d_model=d_model, num_heads=num_heads, dff=dff, vocab_size=input_vocab_size, dropout_rate=dropout_rate)(encoder_input)\n",
        "    decoder = Decoder(num_layers=num_layers, d_model=d_model, num_heads=num_heads, dff=dff, vocab_size=target_vocab_size, dropout_rate=dropout_rate)(decoder_input, encoder)\n",
        "\n",
        "    output = tf.keras.layers.Dense(target_vocab_size)(decoder)\n",
        "\n",
        "    return tf.keras.Model(inputs=inputs, outputs=output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LlERh4N8SrG-",
        "outputId": "66170aea-61f0-44e8-abf6-411915229b50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/7 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-es//opus-100-corpus/v1.0/supervised/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 14%|█▍        | 1/7 [00:00<00:02,  2.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Could not download https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-es//opus-100-corpus/v1.0/supervised/\n",
            "Downloading https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-es/opus.en-es-dev.en\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 29%|██▊       | 2/7 [00:01<00:03,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved opus.en-es-dev.en\n",
            "Downloading https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-es/opus.en-es-dev.es\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 43%|████▎     | 3/7 [00:02<00:02,  1.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved opus.en-es-dev.es\n",
            "Downloading https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-es/opus.en-es-test.en\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 57%|█████▋    | 4/7 [00:02<00:02,  1.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved opus.en-es-test.en\n",
            "Downloading https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-es/opus.en-es-test.es\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 71%|███████▏  | 5/7 [00:03<00:01,  1.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved opus.en-es-test.es\n",
            "Downloading https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-es/opus.en-es-train.en\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 86%|████████▌ | 6/7 [00:06<00:01,  1.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved opus.en-es-train.en\n",
            "Downloading https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-es/opus.en-es-train.es\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7/7 [00:09<00:00,  1.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved opus.en-es-train.es\n",
            "All files have been downloaded.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# URL to the directory containing the files to be downloaded\n",
        "language = \"en-es\"\n",
        "url = f\"https://data.statmt.org/opus-100-corpus/v1.0/supervised/{language}/\"\n",
        "save_directory = f\"./Datasets/{language}\"\n",
        "\n",
        "# Create the save directory if it doesn't exist\n",
        "os.makedirs(save_directory, exist_ok=True)\n",
        "\n",
        "# Send a GET request to the URL\n",
        "response = requests.get(url)\n",
        "\n",
        "# Parse the HTML response\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "# Find all the anchor tags in the HTML\n",
        "links = soup.find_all('a')\n",
        "\n",
        "# Extract the href attribute from each anchor tag\n",
        "file_links = [link['href'] for link in links if '.' in link['href']]\n",
        "\n",
        "# Download each file\n",
        "for file_link in tqdm(file_links):\n",
        "    file_url = url + file_link\n",
        "    save_path = os.path.join(save_directory, file_link)\n",
        "\n",
        "    print(f\"Downloading {file_url}\")\n",
        "\n",
        "    # Send a GET request for the file\n",
        "    file_response = requests.get(file_url)\n",
        "    if file_response.status_code == 404:\n",
        "        print(f\"Could not download {file_url}\")\n",
        "        continue\n",
        "\n",
        "    # Save the file to the specified directory\n",
        "    with open(save_path, 'wb') as file:\n",
        "        file.write(file_response.content)\n",
        "\n",
        "    print(f\"Saved {file_link}\")\n",
        "\n",
        "print(\"All files have been downloaded.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfrdPoh0ToiU",
        "outputId": "0e9f902b-7e51-4339-d0e4-056a092b8902"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60000\n",
            "1990\n",
            "('Fueron los asbestos aquí. ¡Eso es lo que ocurrió!', 'Me voy de aquí.', 'Una vez, juro que cagué una barra de tiza.')\n",
            "(\"It was the asbestos in here, that's what did it!\", \"I'm out of here.\", 'One time, I swear I pooped out a stick of chalk.')\n"
          ]
        }
      ],
      "source": [
        "en_training_data_path = \"Datasets/en-es/opus.en-es-train.en\"\n",
        "en_validation_data_path = \"Datasets/en-es/opus.en-es-dev.en\"\n",
        "es_training_data_path = \"Datasets/en-es/opus.en-es-train.es\"\n",
        "es_validation_data_path = \"Datasets/en-es/opus.en-es-dev.es\"\n",
        "\n",
        "def read_files(path):\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        en_train_dataset = f.read().split(\"\\n\")[:-1]\n",
        "    return en_train_dataset\n",
        "\n",
        "en_training_data = read_files(en_training_data_path)\n",
        "en_validation_data = read_files(en_validation_data_path)\n",
        "es_training_data = read_files(es_training_data_path)\n",
        "es_validation_data = read_files(es_validation_data_path)\n",
        "\n",
        "max_lenght = 500\n",
        "train_dataset = [[es_sentence, en_sentence] for es_sentence, en_sentence in zip(es_training_data, en_training_data) if len(es_sentence) <= max_lenght and len(en_sentence) <= max_lenght]\n",
        "val_dataset = [[es_sentence, en_sentence] for es_sentence, en_sentence in zip(es_validation_data, en_validation_data) if len(es_sentence) <= max_lenght and len(en_sentence) <= max_lenght]\n",
        "es_training_data, en_training_data = zip(*train_dataset)\n",
        "es_validation_data, en_validation_data = zip(*val_dataset)\n",
        "\n",
        "es_training_data, en_training_data = es_training_data[:60000], en_training_data[:60000]\n",
        "\n",
        "print(len(es_training_data))\n",
        "print(len(es_validation_data))\n",
        "print(es_training_data[:3])\n",
        "print(en_training_data[:3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "i4C6qMhPYN0A"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import json\n",
        "import typing\n",
        "from tqdm import tqdm\n",
        "\n",
        "class CustomTokenizer:\n",
        "    \"\"\" Custom Tokenizer class to tokenize and detokenize text data into sequences of integers\n",
        "\n",
        "    Args:\n",
        "        split (str, optional): Split token to use when tokenizing text. Defaults to \" \".\n",
        "        char_level (bool, optional): Whether to tokenize at character level. Defaults to False.\n",
        "        lower (bool, optional): Whether to convert text to lowercase. Defaults to True.\n",
        "        start_token (str, optional): Start token to use when tokenizing text. Defaults to \"<start>\".\n",
        "        end_token (str, optional): End token to use when tokenizing text. Defaults to \"<eos>\".\n",
        "        filters (list, optional): List of characters to filter out. Defaults to\n",
        "            ['!', \"'\", '\"', '#', '$', '%', '&', '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>',\n",
        "            '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~', '\\t', '\\n'].\n",
        "        filter_nums (bool, optional): Whether to filter out numbers. Defaults to True.\n",
        "        start (int, optional): Index to start tokenizing from. Defaults to 1.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "            self,\n",
        "            split: str=\" \",\n",
        "            char_level: bool=False,\n",
        "            lower: bool=True,\n",
        "            start_token: str=\"<start>\",\n",
        "            end_token: str=\"<eos>\",\n",
        "            filters: list = ['!', \"'\", '\"', '#', '$', '%', '&', '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~', '\\t', '\\n'],\n",
        "            filter_nums: bool = True,\n",
        "            start: int=1,\n",
        "        ) -> None:\n",
        "        self.split = split\n",
        "        self.char_level = char_level\n",
        "        self.lower = lower\n",
        "        self.index_word = {}\n",
        "        self.word_index = {}\n",
        "        self.max_length = 0\n",
        "        self.start_token = start_token\n",
        "        self.end_token = end_token\n",
        "        self.filters = filters\n",
        "        self.filter_nums = filter_nums\n",
        "        self.start = start\n",
        "\n",
        "    @property\n",
        "    def start_token_index(self):\n",
        "        return self.word_index[self.start_token]\n",
        "\n",
        "    @property\n",
        "    def end_token_index(self):\n",
        "        return self.word_index[self.end_token]\n",
        "\n",
        "    def sort(self):\n",
        "        \"\"\" Sorts the word_index and index_word dictionaries\"\"\"\n",
        "        self.index_word = dict(enumerate(dict(sorted(self.word_index.items())), start=self.start))\n",
        "        self.word_index = {v: k for k, v in self.index_word.items()}\n",
        "\n",
        "    def split_line(self, line: str):\n",
        "        \"\"\" Splits a line of text into tokens\n",
        "\n",
        "        Args:\n",
        "            line (str): Line of text to split\n",
        "\n",
        "        Returns:\n",
        "            list: List of string tokens\n",
        "        \"\"\"\n",
        "        line = line.lower() if self.lower else line\n",
        "\n",
        "        if self.char_level:\n",
        "            return [char for char in line]\n",
        "\n",
        "        # split line with split token and check for filters\n",
        "        line_tokens = line.split(self.split)\n",
        "\n",
        "        new_tokens = []\n",
        "        for index, token in enumerate(line_tokens):\n",
        "            filtered_tokens = ['']\n",
        "            for c_index, char in enumerate(token):\n",
        "                if char in self.filters or (self.filter_nums and char.isdigit()):\n",
        "                    filtered_tokens += [char, ''] if c_index != len(token) -1 else [char]\n",
        "                else:\n",
        "                    filtered_tokens[-1] += char\n",
        "\n",
        "            new_tokens += filtered_tokens\n",
        "            if index != len(line_tokens) -1:\n",
        "                new_tokens += [self.split]\n",
        "\n",
        "        new_tokens = [token for token in new_tokens if token != '']\n",
        "\n",
        "        return new_tokens\n",
        "\n",
        "    def fit_on_texts(self, lines: typing.List[str]):\n",
        "        \"\"\" Fits the tokenizer on a list of lines of text\n",
        "        This function will update the word_index and index_word dictionaries and set the max_length attribute\n",
        "\n",
        "        Args:\n",
        "            lines (typing.List[str]): List of lines of text to fit the tokenizer on\n",
        "        \"\"\"\n",
        "        self.word_index = {key: value for value, key in enumerate([self.start_token, self.end_token, self.split] + self.filters)}\n",
        "\n",
        "        for line in tqdm(lines, desc=\"Fitting tokenizer\"):\n",
        "            line_tokens = self.split_line(line)\n",
        "            self.max_length = max(self.max_length, len(line_tokens) +2) # +2 for start and end tokens\n",
        "\n",
        "            for token in line_tokens:\n",
        "                if token not in self.word_index:\n",
        "                    self.word_index[token] = len(self.word_index)\n",
        "\n",
        "        self.sort()\n",
        "\n",
        "    def update(self, lines: typing.List[str]):\n",
        "        \"\"\" Updates the tokenizer with new lines of text\n",
        "        This function will update the word_index and index_word dictionaries and set the max_length attribute\n",
        "\n",
        "        Args:\n",
        "            lines (typing.List[str]): List of lines of text to update the tokenizer with\n",
        "        \"\"\"\n",
        "        new_tokens = 0\n",
        "        for line in tqdm(lines, desc=\"Updating tokenizer\"):\n",
        "            line_tokens = self.split_line(line)\n",
        "            self.max_length = max(self.max_length, len(line_tokens) +2) # +2 for start and end tokens\n",
        "            for token in line_tokens:\n",
        "                if token not in self.word_index:\n",
        "                    self.word_index[token] = len(self.word_index)\n",
        "                    new_tokens += 1\n",
        "\n",
        "        self.sort()\n",
        "        print(f\"Added {new_tokens} new tokens\")\n",
        "\n",
        "    def detokenize(self, sequences: typing.List[int], remove_start_end: bool=True):\n",
        "        \"\"\" Converts a list of sequences of tokens back into text\n",
        "\n",
        "        Args:\n",
        "            sequences (typing.list[int]): List of sequences of tokens to convert back into text\n",
        "            remove_start_end (bool, optional): Whether to remove the start and end tokens. Defaults to True.\n",
        "\n",
        "        Returns:\n",
        "            typing.List[str]: List of strings of the converted sequences\n",
        "        \"\"\"\n",
        "        lines = []\n",
        "        for sequence in sequences:\n",
        "            line = \"\"\n",
        "            for token in sequence:\n",
        "                if token == 0:\n",
        "                    break\n",
        "                if remove_start_end and (token == self.start_token_index or token == self.end_token_index):\n",
        "                    continue\n",
        "\n",
        "                line += self.index_word[token]\n",
        "\n",
        "            lines.append(line)\n",
        "\n",
        "        return lines\n",
        "\n",
        "    def texts_to_sequences(self, lines: typing.List[str], include_start_end: bool=True):\n",
        "        \"\"\" Converts a list of lines of text into a list of sequences of tokens\n",
        "\n",
        "        Args:\n",
        "            lines (typing.list[str]): List of lines of text to convert into tokenized sequences\n",
        "            include_start_end (bool, optional): Whether to include the start and end tokens. Defaults to True.\n",
        "\n",
        "        Returns:\n",
        "            typing.List[typing.List[int]]: List of sequences of tokens\n",
        "        \"\"\"\n",
        "        sequences = []\n",
        "        for line in lines:\n",
        "            line_tokens = self.split_line(line)\n",
        "            sequence = [self.word_index[word] for word in line_tokens if word in self.word_index]\n",
        "            if include_start_end:\n",
        "                sequence = [self.word_index[self.start_token]] + sequence + [self.word_index[self.end_token]]\n",
        "\n",
        "            sequences.append(sequence)\n",
        "\n",
        "        return sequences\n",
        "\n",
        "    def save(self, path: str, type: str=\"json\"):\n",
        "        \"\"\" Saves the tokenizer to a file\n",
        "\n",
        "        Args:\n",
        "            path (str): Path to save the tokenizer to\n",
        "            type (str, optional): Type of file to save the tokenizer to. Defaults to \"json\".\n",
        "        \"\"\"\n",
        "        serialised_dict = self.dict()\n",
        "        if type == \"json\":\n",
        "            if os.path.dirname(path):\n",
        "                os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "            with open(path, \"w\") as f:\n",
        "                json.dump(serialised_dict, f)\n",
        "\n",
        "    def dict(self):\n",
        "        \"\"\" Returns a dictionary of the tokenizer\n",
        "\n",
        "        Returns:\n",
        "            dict: Dictionary of the tokenizer\n",
        "        \"\"\"\n",
        "        return {\n",
        "            \"split\": self.split,\n",
        "            \"lower\": self.lower,\n",
        "            \"char_level\": self.char_level,\n",
        "            \"index_word\": self.index_word,\n",
        "            \"max_length\": self.max_length,\n",
        "            \"start_token\": self.start_token,\n",
        "            \"end_token\": self.end_token,\n",
        "            \"filters\": self.filters,\n",
        "            \"filter_nums\": self.filter_nums,\n",
        "            \"start\": self.start\n",
        "        }\n",
        "\n",
        "    @staticmethod\n",
        "    def load(path: typing.Union[str, dict], type: str=\"json\"):\n",
        "        \"\"\" Loads a tokenizer from a file\n",
        "\n",
        "        Args:\n",
        "            path (typing.Union[str, dict]): Path to load the tokenizer from or a dictionary of the tokenizer\n",
        "            type (str, optional): Type of file to load the tokenizer from. Defaults to \"json\".\n",
        "\n",
        "        Returns:\n",
        "            CustomTokenizer: Loaded tokenizer\n",
        "        \"\"\"\n",
        "        if isinstance(path, str):\n",
        "            if type == \"json\":\n",
        "                with open(path, \"r\") as f:\n",
        "                    load_dict = json.load(f)\n",
        "\n",
        "        elif isinstance(path, dict):\n",
        "            load_dict = path\n",
        "\n",
        "        tokenizer = CustomTokenizer()\n",
        "        tokenizer.split = load_dict[\"split\"]\n",
        "        tokenizer.lower = load_dict[\"lower\"]\n",
        "        tokenizer.char_level = load_dict[\"char_level\"]\n",
        "        tokenizer.index_word = {int(k): v for k, v in load_dict[\"index_word\"].items()}\n",
        "        tokenizer.max_length = load_dict[\"max_length\"]\n",
        "        tokenizer.start_token = load_dict[\"start_token\"]\n",
        "        tokenizer.end_token = load_dict[\"end_token\"]\n",
        "        tokenizer.filters = load_dict[\"filters\"]\n",
        "        tokenizer.filter_nums = bool(load_dict[\"filter_nums\"])\n",
        "        tokenizer.start = load_dict[\"start\"]\n",
        "        tokenizer.word_index = {v: int(k) for k, v in tokenizer.index_word.items()}\n",
        "\n",
        "        return tokenizer\n",
        "\n",
        "    @property\n",
        "    def lenght(self):\n",
        "        return len(self.index_word)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.index_word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dnLo0D79YUqe",
        "outputId": "315292a5-fbfd-4f7b-f41f-dab7509c63a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fitting tokenizer: 100%|██████████| 60000/60000 [00:00<00:00, 62914.64it/s]\n",
            "Fitting tokenizer: 100%|██████████| 60000/60000 [00:01<00:00, 49017.47it/s]\n"
          ]
        }
      ],
      "source": [
        "# prepare Spanish tokenizer, this is the input language\n",
        "tokenizer = CustomTokenizer(char_level=True)\n",
        "tokenizer.fit_on_texts(es_training_data)\n",
        "tokenizer.save(\"tokenizer.json\")\n",
        "\n",
        "# prepare English tokenizer, this is the output language\n",
        "detokenizer = CustomTokenizer(char_level=True)\n",
        "detokenizer.fit_on_texts(en_training_data)\n",
        "detokenizer.save(\"detokenizer.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8c90ldFSYXD1",
        "outputId": "e6704abc-804a-4df0-99a5-fa2bd4f326f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:DataProvider:Skipping Dataset validation...\n",
            "INFO:DataProvider:Skipping Dataset validation...\n"
          ]
        }
      ],
      "source": [
        "from mltu.tensorflow.dataProvider import DataProvider\n",
        "import numpy as np\n",
        "\n",
        "def preprocess_inputs(data_batch, label_batch):\n",
        "    encoder_input = np.zeros((len(data_batch), tokenizer.max_length)).astype(np.int64)\n",
        "    decoder_input = np.zeros((len(label_batch), detokenizer.max_length)).astype(np.int64)\n",
        "    decoder_output = np.zeros((len(label_batch), detokenizer.max_length)).astype(np.int64)\n",
        "\n",
        "    data_batch_tokens = tokenizer.texts_to_sequences(data_batch)\n",
        "    label_batch_tokens = detokenizer.texts_to_sequences(label_batch)\n",
        "\n",
        "    for index, (data, label) in enumerate(zip(data_batch_tokens, label_batch_tokens)):\n",
        "        encoder_input[index][:len(data)] = data\n",
        "        decoder_input[index][:len(label)-1] = label[:-1] # Drop the [END] tokens\n",
        "        decoder_output[index][:len(label)-1] = label[1:] # Drop the [START] tokens\n",
        "\n",
        "    return (encoder_input, decoder_input), decoder_output\n",
        "\n",
        "train_dataProvider = DataProvider(\n",
        "    train_dataset,\n",
        "    batch_size=4,\n",
        "    batch_postprocessors=[preprocess_inputs],\n",
        "    use_cache=True\n",
        "    )\n",
        "\n",
        "val_dataProvider = DataProvider(\n",
        "    val_dataset,\n",
        "    batch_size=4,\n",
        "    batch_postprocessors=[preprocess_inputs],\n",
        "    use_cache=True\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "wr7zAk5NYca4"
      },
      "outputs": [],
      "source": [
        "try: [tf.config.experimental.set_memory_growth(gpu, True) for gpu in tf.config.experimental.list_physical_devices(\"GPU\")]\n",
        "except: pass\n",
        "\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
        "from mltu.tensorflow.callbacks import Model2onnx, WarmupCosineDecay\n",
        "\n",
        "from mltu.tensorflow.dataProvider import DataProvider\n",
        "from mltu.tokenizers import CustomTokenizer\n",
        "\n",
        "from mltu.tensorflow.transformer.utils import MaskedAccuracy, MaskedLoss\n",
        "from mltu.tensorflow.transformer.callbacks import EncDecSplitCallback"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "from mltu.configs import BaseModelConfigs\n",
        "\n",
        "\n",
        "class ModelConfigs(BaseModelConfigs):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model_path = os.path.join(\n",
        "            \"Models/translation_transformer\",\n",
        "            datetime.strftime(datetime.now(), \"%Y%m%d%H%M\"),\n",
        "        )\n",
        "        self.num_layers = 4\n",
        "        self.d_model = 128\n",
        "        self.num_heads = 8\n",
        "        self.dff = 512\n",
        "        self.dropout_rate = 0.1\n",
        "        self.batch_size = 16\n",
        "        self.train_epochs = 50\n",
        "        # CustomSchedule parameters\n",
        "        self.init_lr = 0.00001\n",
        "        self.lr_after_warmup = 0.0005\n",
        "        self.final_lr = 0.0001\n",
        "        self.warmup_epochs = 2\n",
        "        self.decay_epochs = 18"
      ],
      "metadata": {
        "id": "ZyeYRx3jMLMx"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create TensorFlow Transformer Model\n",
        "configs = ModelConfigs()\n",
        "transformer = Transformer(\n",
        "    num_layers=configs.num_layers,\n",
        "    d_model=configs.d_model,\n",
        "    num_heads=configs.num_heads,\n",
        "    dff=configs.dff,\n",
        "    input_vocab_size=len(tokenizer)+1,\n",
        "    target_vocab_size=len(detokenizer)+1,\n",
        "    dropout_rate=configs.dropout_rate,\n",
        "    encoder_input_size=tokenizer.max_length,\n",
        "    decoder_input_size=detokenizer.max_length\n",
        "    )\n",
        "\n",
        "transformer.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cBpJacOZMOuF",
        "outputId": "1a95a95f-37b1-469b-abf2-6b92cd779192"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None, 502)]                0         []                            \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)        [(None, 502)]                0         []                            \n",
            "                                                                                                  \n",
            " encoder_1 (Encoder)         (None, 502, 128)             2672256   ['input_1[0][0]']             \n",
            "                                                                                                  \n",
            " decoder_1 (Decoder)         (None, 502, 128)             4788480   ['input_2[0][0]',             \n",
            "                                                                     'encoder_1[0][0]']           \n",
            "                                                                                                  \n",
            " dense_30 (Dense)            (None, 502, 302)             38958     ['decoder_1[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 7499694 (28.61 MB)\n",
            "Trainable params: 7499694 (28.61 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=configs.init_lr, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
        "\n",
        "# Compile the model\n",
        "transformer.compile(\n",
        "    loss=MaskedLoss(),\n",
        "    optimizer=optimizer,\n",
        "    metrics=[MaskedAccuracy()],\n",
        "    run_eagerly=False\n",
        "    )"
      ],
      "metadata": {
        "id": "dgFQzr7DNAj4"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tf2onnx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9Xj49G0OZCy",
        "outputId": "86c55006-ffb2-4c86-c957-7bb8dc669fdf"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tf2onnx in /usr/local/lib/python3.10/dist-packages (1.15.1)\n",
            "Requirement already satisfied: numpy>=1.14.1 in /usr/local/lib/python3.10/dist-packages (from tf2onnx) (1.23.5)\n",
            "Requirement already satisfied: onnx>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from tf2onnx) (1.15.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from tf2onnx) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from tf2onnx) (1.16.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.10/dist-packages (from tf2onnx) (23.5.26)\n",
            "Requirement already satisfied: protobuf~=3.20.2 in /usr/local/lib/python3.10/dist-packages (from tf2onnx) (3.20.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->tf2onnx) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->tf2onnx) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->tf2onnx) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->tf2onnx) (2023.7.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define callbacks\n",
        "warmupCosineDecay = WarmupCosineDecay(\n",
        "    lr_after_warmup=configs.lr_after_warmup,\n",
        "    final_lr=configs.final_lr,\n",
        "    warmup_epochs=configs.warmup_epochs,\n",
        "    decay_epochs=configs.decay_epochs,\n",
        "    initial_lr=configs.init_lr,\n",
        "    )\n",
        "earlystopper = EarlyStopping(monitor=\"val_masked_accuracy\", patience=5, verbose=1, mode=\"max\")\n",
        "checkpoint = ModelCheckpoint(f\"{configs.model_path}/model.h5\", monitor=\"val_masked_accuracy\", verbose=1, save_best_only=True, mode=\"max\", save_weights_only=False)\n",
        "tb_callback = TensorBoard(f\"{configs.model_path}/logs\")\n",
        "reduceLROnPlat = ReduceLROnPlateau(monitor=\"val_masked_accuracy\", factor=0.9, min_delta=1e-10, patience=2, verbose=1, mode=\"max\")\n",
        "model2onnx = Model2onnx(f\"{configs.model_path}/model.h5\", metadata={\"tokenizer\": tokenizer.dict(), \"detokenizer\": detokenizer.dict()}, save_on_epoch_end=False)\n",
        "encDecSplitCallback = EncDecSplitCallback(configs.model_path, encoder_metadata={\"tokenizer\": tokenizer.dict()}, decoder_metadata={\"detokenizer\": detokenizer.dict()})"
      ],
      "metadata": {
        "id": "2G5eeoKYNuEU"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "transformer.fit(\n",
        "    train_dataProvider,\n",
        "    validation_data=val_dataProvider,\n",
        "    epochs=configs.train_epochs,\n",
        "    callbacks=[\n",
        "        warmupCosineDecay,\n",
        "        checkpoint,\n",
        "        tb_callback,\n",
        "        reduceLROnPlat,\n",
        "        model2onnx,\n",
        "        encDecSplitCallback\n",
        "        ]\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        },
        "id": "gUZyKmOfNw70",
        "outputId": "841b7087-e2f8-4307-8f12-7bfe4dcceaa8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-9728fe244b09>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m transformer.fit(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mtrain_dataProvider\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_dataProvider\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfigs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'transformer' is not defined"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPuj/Qk5d9NyjxkBftAZvQe"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}